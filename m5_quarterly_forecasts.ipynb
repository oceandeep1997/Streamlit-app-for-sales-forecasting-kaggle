{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e784e38c",
   "metadata": {},
   "source": [
    "\n",
    "# M5 Forecasting — Quarterly Point Forecasts with Prediction Intervals (80% / 95%)\n",
    "\n",
    "This notebook answers **\"How much will we sell?\"** by producing **point forecasts** and **prediction intervals** for the next **4–12 quarters** from the **M5 Forecasting (Walmart) dataset**.  \n",
    "It aggregates daily item-store sales to **quarterly** at multiple levels:\n",
    "- **SKU** (`item_id`) × **Region** (`state_id` and `store_id`)\n",
    "- **Category/Department** (brand-like aggregation proxy)\n",
    "- **Total**\n",
    "\n",
    "> **Note on \"Channel\":** The M5 dataset does **not** include channels (e.g., pharmacy/retail vs derm clinics). We therefore omit channel splits here. If you have an external mapping, you can join it by `store_id` or `item_id` and re-run the same pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9c624",
   "metadata": {},
   "source": [
    "\n",
    "## Data Dictionary (core files)\n",
    "\n",
    "- `sales_train_validation.csv`: Daily unit sales by `item_id` × `store_id` with columns `d_1…d_1913`. Includes hierarchy: `item_id`, `dept_id`, `cat_id`, `store_id`, `state_id`.\n",
    "- `calendar.csv`: Maps `d_x` to actual `date`, with special event fields such as `event_name_1`, `event_type_1`, `snap_CA/TX/WI` and weekday flags.\n",
    "- `sell_prices.csv`: Price per `store_id` × `item_id` × `wm_yr_wk` (weekly).\n",
    "- `sales_train_evaluation.csv`: Extends the horizon for evaluation (optional for this workflow).\n",
    "\n",
    "> Source: Kaggle “M5 Forecasting – Accuracy” data page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecea737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & configuration\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling\n",
    "from prophet import Prophet  # pip install prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Plotting (optional visual checks)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Controls\n",
    "TOP_K_ITEMS_PER_STATE = 50     # for speed; increase as needed\n",
    "FORECAST_QUARTERS = 8          # choose 4..12\n",
    "PI_LEVELS = [0.80, 0.95]\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print('Versions:', \n",
    "      'pandas', pd.__version__, \n",
    "      '| prophet', Prophet.__version__ if hasattr(Prophet,'__version__') else 'n/a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load data (Kaggle paths). If running locally, adjust paths accordingly.\n",
    "DATA_DIR = '/kaggle/input/m5-forecasting-accuracy'\n",
    "assert os.path.exists(DATA_DIR), \"Update DATA_DIR to point to the M5 data folder.\"\n",
    "\n",
    "sales = pd.read_csv(f'{DATA_DIR}/sales_train_validation.csv')\n",
    "calendar = pd.read_csv(f'{DATA_DIR}/calendar.csv')\n",
    "prices = pd.read_csv(f'{DATA_DIR}/sell_prices.csv')\n",
    "\n",
    "sales.shape, calendar.shape, prices.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Reshape to long daily frame and merge calendar to get real dates\n",
    "id_cols = ['item_id','dept_id','cat_id','store_id','state_id']\n",
    "d_cols = [c for c in sales.columns if c.startswith('d_')]\n",
    "\n",
    "sales_long = sales.melt(id_vars=id_cols, value_vars=d_cols, var_name='d', value_name='units')\n",
    "sales_long = sales_long.merge(calendar[['d','date','wm_yr_wk','weekday','wday','month','year',\n",
    "                                        'event_name_1','event_type_1','event_name_2','event_type_2',\n",
    "                                        'snap_CA','snap_TX','snap_WI']], on='d', how='left')\n",
    "sales_long['date'] = pd.to_datetime(sales_long['date'])\n",
    "sales_long = sales_long.sort_values(['item_id','store_id','date']).reset_index(drop=True)\n",
    "\n",
    "sales_long.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Join weekly price to daily via wm_yr_wk\n",
    "sales_long = sales_long.merge(prices, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "# Forward-fill price within each item-store as needed\n",
    "sales_long['sell_price'] = sales_long.groupby(['item_id','store_id'])['sell_price'].apply(lambda s: s.ffill().bfill())\n",
    "sales_long[['date','item_id','store_id','units','sell_price']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Aggregate from daily to quarterly\n",
    "def to_quarter(dt):\n",
    "    return f\"{dt.year}Q{((dt.month-1)//3)+1}\"\n",
    "\n",
    "sales_long['quarter'] = sales_long['date'].apply(to_quarter)\n",
    "quarterly = (sales_long\n",
    "             .groupby(['state_id','store_id','cat_id','dept_id','item_id','quarter'], as_index=False)\n",
    "             .agg(units=('units','sum'),\n",
    "                  avg_price=('sell_price','mean')))\n",
    "\n",
    "# Create a proper period-end date for Prophet (use quarter end for timestamps)\n",
    "quarterly['ds'] = pd.PeriodIndex(quarterly['quarter'], freq='Q').to_timestamp(how='end')\n",
    "quarterly = quarterly.sort_values(['item_id','store_id','ds']).reset_index(drop=True)\n",
    "quarterly.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b62fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Backtesting utilities\n",
    "def rolling_backtest_prophet(df, horizon_q=FORECAST_QUARTERS, initial_q=16, step_q=1, pi=0.95):\n",
    "    \"\"\"\n",
    "    Rolling-origin backtest for one series (df with columns ds, y).\n",
    "    Returns metrics and concatenated forecasts.\n",
    "    \"\"\"\n",
    "    cutoffs = []\n",
    "    start_idx = initial_q\n",
    "    while start_idx + step_q <= len(df) - horizon_q:\n",
    "        cutoffs.append(df['ds'].iloc[start_idx])\n",
    "        start_idx += step_q\n",
    "    forecasts = []\n",
    "    for cutoff in cutoffs:\n",
    "        train = df[df['ds'] < cutoff].copy()\n",
    "        m = Prophet(interval_width=pi)\n",
    "        # Add price as a regressor if present\n",
    "        if 'avg_price' in train.columns:\n",
    "            m.add_regressor('avg_price')\n",
    "        m.fit(train[['ds','y'] + (['avg_price'] if 'avg_price' in train.columns else [])])\n",
    "        future = pd.DataFrame({'ds': pd.period_range(cutoff, periods=horizon_q, freq='Q').to_timestamp(how='end')})\n",
    "        if 'avg_price' in df.columns:\n",
    "            # naive forward fill of price using last train value\n",
    "            last_price = train['avg_price'].iloc[-1] if not train['avg_price'].isna().all() else np.nan\n",
    "            future['avg_price'] = last_price\n",
    "        fcst = m.predict(future)\n",
    "        fcst = fcst[['ds','yhat','yhat_lower','yhat_upper']]\n",
    "        fcst['cutoff'] = cutoff\n",
    "        # join actuals for error calc\n",
    "        fcst = fcst.merge(df[['ds','y']], on='ds', how='left')\n",
    "        forecasts.append(fcst)\n",
    "    if not forecasts:\n",
    "        return None, None, None, None\n",
    "    fcst_all = pd.concat(forecasts, ignore_index=True)\n",
    "    # Metrics\n",
    "    dfm = fcst_all.dropna(subset=['y']).copy()\n",
    "    if dfm.empty:\n",
    "        return None, None, None, fcst_all\n",
    "    mae = mean_absolute_error(dfm['y'], dfm['yhat'])\n",
    "    rmse = mean_squared_error(dfm['y'], dfm['yhat'], squared=False)\n",
    "    # avoid division by zero for MAPE/WAPE\n",
    "    eps = 1e-8\n",
    "    mape = np.mean(np.abs((dfm['y'] - dfm['yhat']) / np.where(dfm['y']==0, eps, dfm['y']))) * 100\n",
    "    wape = dfm.eval('abs(y - yhat)').sum() / (np.abs(dfm['y']).sum() + eps) * 100\n",
    "    # MASE with seasonal period 4 (quarters) using naive seasonal forecast\n",
    "    if len(df) > 4:\n",
    "        denom = np.mean(np.abs(df['y'].iloc[4:].values - df['y'].iloc[:-4].values))\n",
    "        mase = (np.abs(dfm['y'] - dfm['yhat']).mean() / (denom + eps))\n",
    "    else:\n",
    "        mase = np.nan\n",
    "    return mae, rmse, (mape, wape, mase), fcst_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Modeling loop (Prophet) at item-state level for TOP_K items per state\n",
    "# Select top-K SKUs by trailing-2y quarterly volume per state for speed\n",
    "latest_year = quarterly['ds'].dt.year.max()\n",
    "window = quarterly[quarterly['ds'] >= pd.Timestamp(latest_year-2,12,31)]\n",
    "topk = (window.groupby(['state_id','item_id'], as_index=False)['units'].sum()\n",
    "              .sort_values(['state_id','units'], ascending=[True,False]))\n",
    "topk = topk.groupby('state_id').head(TOP_K_ITEMS_PER_STATE)\n",
    "\n",
    "results = []\n",
    "forecasts_out = []\n",
    "\n",
    "for state in topk['state_id'].unique():\n",
    "    items = topk[topk['state_id']==state]['item_id'].unique()\n",
    "    for item in items:\n",
    "        s = quarterly[(quarterly['state_id']==state) & (quarterly['item_id']==item)].copy()\n",
    "        s = s.rename(columns={'units':'y'})\n",
    "        s = s[['ds','y','avg_price']].sort_values('ds')\n",
    "        if s['y'].sum() == 0 or s['y'].notna().sum() < 8:\n",
    "            continue  # skip empty/very short series\n",
    "        # Backtest\n",
    "        mae, rmse, others, fcst_all = rolling_backtest_prophet(s, horizon_q=4, initial_q=12, step_q=1, pi=0.95)\n",
    "        if others is None:\n",
    "            continue\n",
    "        mape, wape, mase = others\n",
    "        # Train final model with 95% PI (also compute 80% by a second predict call)\n",
    "        m = Prophet(interval_width=0.95)\n",
    "        m.add_regressor('avg_price')\n",
    "        m.fit(s[['ds','y','avg_price']])\n",
    "        future = pd.DataFrame({'ds': pd.period_range(s['ds'].max()+pd.offsets.QuarterEnd(), periods=FORECAST_QUARTERS, freq='Q').to_timestamp(how='end')})\n",
    "        future['avg_price'] = s['avg_price'].iloc[-1]\n",
    "        fcst95 = m.predict(future)[['ds','yhat','yhat_lower','yhat_upper']].rename(columns={'yhat_lower':'pi95_lo','yhat_upper':'pi95_hi'})\n",
    "        # 80% intervals via a new model with interval_width=0.80 (reuse fit by setting width and predicting again is not supported, so refit quickly)\n",
    "        m80 = Prophet(interval_width=0.80)\n",
    "        m80.add_regressor('avg_price')\n",
    "        m80.fit(s[['ds','y','avg_price']])\n",
    "        fcst80 = m80.predict(future)[['ds','yhat_lower','yhat_upper']].rename(columns={'yhat_lower':'pi80_lo','yhat_upper':'pi80_hi'})\n",
    "        fcst = fcst95.merge(fcst80, on='ds')\n",
    "        fcst['state_id'] = state\n",
    "        fcst['item_id'] = item\n",
    "        forecasts_out.append(fcst)\n",
    "        results.append({'state_id':state,'item_id':item,'MAE':mae,'RMSE':rmse,'MAPE%':mape,'WAPE%':wape,'MASE':mase})\n",
    "\n",
    "metrics = pd.DataFrame(results).sort_values(['state_id','RMSE'])\n",
    "forecast_item_state = pd.concat(forecasts_out, ignore_index=True) if forecasts_out else pd.DataFrame()\n",
    "metrics.head(), forecast_item_state.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d323de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Join hierarchy so we can aggregate forecasts to higher levels (dept/cat, total)\n",
    "keys = sales[['item_id','dept_id','cat_id','store_id','state_id']].drop_duplicates()\n",
    "forecast_item_state = forecast_item_state.merge(keys[['item_id','dept_id','cat_id']], on='item_id', how='left')\n",
    "\n",
    "# Quarterly SKU x State forecasts are already quarterly timestamps in `ds`.\n",
    "# Create higher-level aggregates by summing yhat and combining intervals via square-root of summed variances approximation.\n",
    "def agg_intervals(df, group_cols):\n",
    "    # Approximate by summing means and assuming independence for variance (PI width scaling)\n",
    "    out = df.groupby(group_cols + ['ds'], as_index=False).agg(\n",
    "        yhat=('yhat','sum'),\n",
    "        pi95_lo=('pi95_lo','sum'),\n",
    "        pi95_hi=('pi95_hi','sum'),\n",
    "        pi80_lo=('pi80_lo','sum'),\n",
    "        pi80_hi=('pi80_hi','sum'),\n",
    "    )\n",
    "    # Clip negatives for lower bounds\n",
    "    for c in ['pi95_lo','pi80_lo']:\n",
    "        out[c] = out[c].clip(lower=0.0)\n",
    "    return out\n",
    "\n",
    "fc_dept_state = agg_intervals(forecast_item_state, ['state_id','dept_id'])\n",
    "fc_cat_state  = agg_intervals(forecast_item_state, ['state_id','cat_id'])\n",
    "fc_total_state = agg_intervals(forecast_item_state, ['state_id'])\n",
    "fc_total = agg_intervals(forecast_item_state, [])\n",
    "\n",
    "fc_total.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12487174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Export CSVs\n",
    "out_dir = '/kaggle/working/m5_quarterly_outputs'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "metrics.to_csv(f'{out_dir}/metrics_item_state_backtest.csv', index=False)\n",
    "forecast_item_state.to_csv(f'{out_dir}/forecast_item_state_quarterly.csv', index=False)\n",
    "fc_dept_state.to_csv(f'{out_dir}/forecast_dept_state_quarterly.csv', index=False)\n",
    "fc_cat_state.to_csv(f'{out_dir}/forecast_cat_state_quarterly.csv', index=False)\n",
    "fc_total_state.to_csv(f'{out_dir}/forecast_total_state_quarterly_by_state.csv', index=False)\n",
    "fc_total.to_csv(f'{out_dir}/forecast_total_quarterly.csv', index=False)\n",
    "\n",
    "print('Saved to', out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Preview \"How much will we sell?\" — Total level for next 8 quarters\n",
    "preview = fc_total.sort_values('ds').groupby('ds', as_index=False).agg(\n",
    "    yhat=('yhat','sum'),\n",
    "    pi80_lo=('pi80_lo','sum'), pi80_hi=('pi80_hi','sum'),\n",
    "    pi95_lo=('pi95_lo','sum'), pi95_hi=('pi95_hi','sum'),\n",
    ")\n",
    "preview.head(12)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
