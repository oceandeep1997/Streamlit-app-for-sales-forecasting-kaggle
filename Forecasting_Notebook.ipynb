{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aaeae2",
   "metadata": {},
   "source": [
    "\n",
    "# Retail Forecasting & Finance Notebook\n",
    "This notebook supports **Rossmann Store Sales** and **Corporación Favorita Grocery Sales** datasets and mirrors the one-hour assessment workflow:\n",
    "- Forecast (with 80/95% intervals) across **Store × Family** (proxy for SKU/brand family), region, channel (retail), next **4–12 quarters** (weekly horizon).\n",
    "- Explain **drivers** (promotions, holidays, macro oil, calendar/seasonality).\n",
    "- Evaluate reliability (**WAPE, MAPE, MASE, RMSE, SMAPE**).\n",
    "- Run **scenarios**: **BASE**, **PROMO**, **+2% PRICE**, **SUPPLY CAP**.\n",
    "- Translate to **finance**: Gross→**GTN**→Net, **COGS**, **GM**, **Contribution**; **Revenue Bridge**.\n",
    "- **Promo ROI** and **Inventory** (safety stock).\n",
    "- Auto-generate a **12-slide PowerPoint** summary.\n",
    "\n",
    "> Tip: Use Favorita for richer exogenous signals; Rossmann is simpler but close to OTC/beauty retail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5963ff",
   "metadata": {},
   "source": [
    "## Run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609a12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose dataset: 'rossmann' or 'favorita'\n",
    "DATASET = 'favorita'  # or 'rossmann'\n",
    "\n",
    "# For Rossmann\n",
    "ROSS_TRAIN = './rossmann/train.csv'\n",
    "ROSS_STORE = './rossmann/store.csv'\n",
    "\n",
    "# For Favorita (folder with CSVs or .7z files)\n",
    "FAVORITA_DIR = './favorita-grocery-sales-forecasting'\n",
    "FAVORITA_SINCE = '2015-01-01'  # optional filter for faster prototyping\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = './outputs_notebook'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Finance assumptions (tweak per business)\n",
    "GTN_DEFAULT = 0.18       # Gross-to-Net rate\n",
    "COGS_DEFAULT = 0.45      # COGS % of Net Revenue\n",
    "PROMO_COST_RATE = 0.20   # % incremental NR used as promo cost\n",
    "LEAD_WEEKS = {'AESTHETICS': 8, 'RX_DERM': 6, 'CONSUMER': 4}\n",
    "SERVICE_LEVEL_Z = 1.65   # ~95%\n",
    "\n",
    "# Forecast horizon (weeks) ~ 52 = 4 quarters\n",
    "HORIZON_WEEKS = 52\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e1ade",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e891cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    import numpy as np\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    out = np.where(denom==0, 0, np.abs(y_true - y_pred) / denom)\n",
    "    return float(np.mean(out) * 100)\n",
    "\n",
    "def mase(y_true, y_pred, y_train_hist):\n",
    "    import numpy as np\n",
    "    if len(y_train_hist) < 2:\n",
    "        return float('nan')\n",
    "    denom = np.mean(np.abs(np.diff(y_train_hist)))\n",
    "    if denom == 0:\n",
    "        return float('nan')\n",
    "    return float(np.mean(np.abs(y_true - y_pred)) / denom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5ac2c",
   "metadata": {},
   "source": [
    "## Data loading — Rossmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fde959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_rossmann(train_path: str, store_path: str) -> pd.DataFrame:\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"Date\"])\n",
    "    store = pd.read_csv(store_path)\n",
    "    df = train.merge(store, on=\"Store\", how=\"left\")\n",
    "    df = df[(df[\"Open\"]==1) & (df[\"Sales\"]>0)].copy()\n",
    "    df[\"channel\"] = \"RETAIL\"\n",
    "    df[\"region\"] = df[\"StoreType\"].fillna(\"U\")\n",
    "    return df\n",
    "\n",
    "def add_calendar_features(df: pd.DataFrame, date_col=\"Date\", daily=True) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[\"year\"] = d[date_col].dt.year\n",
    "    d[\"month\"] = d[date_col].dt.month\n",
    "    d[\"week\"] = d[date_col].dt.isocalendar().week.astype(int)\n",
    "    d[\"dow\"] = d[date_col].dt.weekday\n",
    "    d[\"is_month_end\"] = d[date_col].dt.is_month_end.astype(int)\n",
    "    d[\"is_q_end\"] = d[date_col].dt.is_quarter_end.astype(int)\n",
    "    if daily:\n",
    "        d[\"day\"] = d[date_col].dt.day\n",
    "        d[\"is_weekend\"] = d[\"dow\"].isin([5,6]).astype(int)\n",
    "    return d\n",
    "\n",
    "def add_promo_history(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = d.sort_values([\"Store\",\"Date\"]).copy()\n",
    "    d[\"promo_lag1\"] = d.groupby(\"Store\")[\"Promo\"].shift(1).fillna(0)\n",
    "    d[\"promo_rolling_4w\"] = d.groupby(\"Store\")[\"Promo\"].transform(lambda s: s.rolling(28, min_periods=1).mean())\n",
    "    comp_year = d[\"CompetitionOpenSinceYear\"].fillna(0).astype(int)\n",
    "    comp_month = d[\"CompetitionOpenSinceMonth\"].fillna(1).astype(int)\n",
    "    comp_date = pd.to_datetime(comp_year.astype(str) + \"-\" + comp_month.astype(str) + \"-01\", errors=\"coerce\")\n",
    "    d[\"comp_days\"] = (d[\"Date\"] - comp_date).dt.days.fillna(0).clip(lower=0)\n",
    "    d[\"Promo2Since\"] = pd.to_datetime(d[\"Promo2SinceYear\"].fillna(0).astype(int).astype(str) + \"-\" +\n",
    "                                      d[\"Promo2SinceWeek\"].fillna(1).astype(int).astype(str) + \"-1\", errors=\"coerce\")\n",
    "    d[\"promo2_days\"] = (d[\"Date\"] - d[\"Promo2Since\"]).dt.days.fillna(0).clip(lower=0)\n",
    "    return d\n",
    "\n",
    "def weekly_panel_rossmann(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = (df\n",
    "         .assign(week_start=df[\"Date\"] - pd.to_timedelta(df[\"Date\"].dt.weekday, unit=\"D\"))\n",
    "         .groupby([\"Store\",\"week_start\",\"region\",\"channel\",\"StoreType\",\"Assortment\"], as_index=False)\n",
    "         .agg(Sales=(\"Sales\",\"sum\"),\n",
    "              Customers=(\"Customers\",\"sum\"),\n",
    "              Promo=(\"Promo\",\"max\"),\n",
    "              SchoolHoliday=(\"SchoolHoliday\",\"max\"),\n",
    "              StateHoliday=(\"StateHoliday\",\"max\"),\n",
    "              comp_days=(\"comp_days\",\"max\"),\n",
    "              promo2_days=(\"promo2_days\",\"max\")))\n",
    "    g[\"price_index\"] = (g.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.rolling(8, min_periods=1).mean()) /\n",
    "                        g.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.rolling(26, min_periods=1).mean())).fillna(1.0).clip(0.5, 1.5)\n",
    "    map_family = {\"a\":\"CONSUMER\",\"b\":\"RX_DERM\",\"c\":\"AESTHETICS\"}\n",
    "    g[\"family\"] = g[\"StoreType\"].str.lower().map(map_family).fillna(\"CONSUMER\")\n",
    "    return g.rename(columns={\"week_start\":\"date\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35cbf9",
   "metadata": {},
   "source": [
    "## Data loading — Favorita (.7z aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792463be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_7z_if_needed(path: str, out_dir: str) -> str:\n",
    "    if path.lower().endswith(\".7z\"):\n",
    "        try:\n",
    "            import py7zr\n",
    "        except ImportError:\n",
    "            raise SystemExit(\"Install py7zr to open .7z files: pip install py7zr\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        with py7zr.SevenZipFile(path, mode='r') as z:\n",
    "            z.extractall(path=out_dir)\n",
    "        for f in os.listdir(out_dir):\n",
    "            if f.lower().endswith(\".csv\"):\n",
    "                return os.path.join(out_dir, f)\n",
    "        raise FileNotFoundError(f\"No CSV found after extracting {path}\")\n",
    "    return path\n",
    "\n",
    "def resolve_in_dir(data_dir: str, name: str) -> str:\n",
    "    csv = os.path.join(data_dir, name.replace(\".7z\",\"\"))\n",
    "    if os.path.exists(csv):\n",
    "        return csv\n",
    "    seven = os.path.join(data_dir, name)\n",
    "    if os.path.exists(seven):\n",
    "        out_dir = os.path.join(data_dir, \"_extracted\", os.path.splitext(name)[0])\n",
    "        return extract_7z_if_needed(seven, out_dir)\n",
    "    raise FileNotFoundError(f\"Missing {name}(.7z) in {data_dir}\")\n",
    "\n",
    "def favor_dtypes(file: str) -> Dict[str, str]:\n",
    "    if file.endswith(\"train.csv\"):\n",
    "        return dict(store_nbr=\"int16\", item_nbr=\"int32\", onpromotion=\"boolean\")\n",
    "    if file.endswith(\"stores.csv\"):\n",
    "        return dict(store_nbr=\"int16\", city=\"category\", state=\"category\", type=\"category\", cluster=\"int16\")\n",
    "    if file.endswith(\"transactions.csv\"):\n",
    "        return dict(store_nbr=\"int16\", transactions=\"int32\")\n",
    "    if file.endswith(\"oil.csv\"):\n",
    "        return dict(dcoilwtico=\"float32\")\n",
    "    return {}\n",
    "\n",
    "def load_favorita(data_dir: str, since: str = None) -> pd.DataFrame:\n",
    "    train_path = resolve_in_dir(data_dir, \"train.csv.7z\")\n",
    "    items_path = resolve_in_dir(data_dir, \"items.csv.7z\")\n",
    "    stores_path = resolve_in_dir(data_dir, \"stores.csv.7z\")\n",
    "    holidays_path = resolve_in_dir(data_dir, \"holidays_events.csv.7z\")\n",
    "    oil_path = resolve_in_dir(data_dir, \"oil.csv.7z\")\n",
    "    trans_path = resolve_in_dir(data_dir, \"transactions.csv.7z\")\n",
    "\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"date\"], dtype=favor_dtypes(\"train.csv\"))\n",
    "    items = pd.read_csv(items_path).rename(columns={\"class\":\"_class\"})\n",
    "    stores = pd.read_csv(stores_path, dtype=favor_dtypes(\"stores.csv\"))\n",
    "    holidays = pd.read_csv(holidays_path, parse_dates=[\"date\"])\n",
    "    oil = pd.read_csv(oil_path, parse_dates=[\"date\"], dtype=favor_dtypes(\"oil.csv\"))\n",
    "    trans = pd.read_csv(trans_path, parse_dates=[\"date\"], dtype=favor_dtypes(\"transactions.csv\"))\n",
    "\n",
    "    if since:\n",
    "        dt = pd.to_datetime(since)\n",
    "        train = train[train[\"date\"] >= dt]\n",
    "        trans = trans[trans[\"date\"] >= dt]\n",
    "        oil = oil[oil[\"date\"] >= dt]\n",
    "        holidays = holidays[holidays[\"date\"] >= dt]\n",
    "\n",
    "    df = (train.merge(items, on=\"item_nbr\", how=\"left\")\n",
    "                .merge(stores, on=\"store_nbr\", how=\"left\"))\n",
    "    df[\"week_start\"] = df[\"date\"] - pd.to_timedelta(df[\"date\"].dt.weekday, unit=\"D\")\n",
    "    agg = (df.groupby([\"store_nbr\",\"family\",\"week_start\",\"type\",\"cluster\",\"city\",\"state\"], as_index=False)\n",
    "             .agg(unit_sales=(\"unit_sales\",\"sum\"),\n",
    "                  onpromo=(\"onpromotion\",\"mean\"),\n",
    "                  perishable_rate=(\"perishable\",\"mean\")))\n",
    "\n",
    "    trans[\"week_start\"] = trans[\"date\"] - pd.to_timedelta(trans[\"date\"].dt.weekday, unit=\"D\")\n",
    "    trans_w = trans.groupby([\"store_nbr\",\"week_start\"], as_index=False)[\"transactions\"].sum()\n",
    "    agg = agg.merge(trans_w, on=[\"store_nbr\",\"week_start\"], how=\"left\")\n",
    "\n",
    "    oil[\"week_start\"] = oil[\"date\"] - pd.to_timedelta(oil[\"date\"].dt.weekday, unit=\"D\")\n",
    "    oil_w = oil.groupby(\"week_start\", as_index=False)[\"dcoilwtico\"].mean()\n",
    "    agg = agg.merge(oil_w, on=\"week_start\", how=\"left\")\n",
    "\n",
    "    holidays[\"is_holiday\"] = 1\n",
    "    hol_w = holidays.groupby(\"date\", as_index=False)[\"is_holiday\"].max()\n",
    "    hol_w[\"week_start\"] = hol_w[\"date\"] - pd.to_timedelta(hol_w[\"date\"].dt.weekday, unit=\"D\")\n",
    "    hol_w = hol_w.groupby(\"week_start\", as_index=False)[\"is_holiday\"].max()\n",
    "    agg = agg.merge(hol_w, on=\"week_start\", how=\"left\")\n",
    "    agg[\"is_holiday\"] = agg[\"is_holiday\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "    panel = agg.rename(columns={\n",
    "        \"week_start\":\"date\",\n",
    "        \"store_nbr\":\"Store\",\n",
    "        \"unit_sales\":\"Sales\",\n",
    "        \"onpromo\":\"Promo\"\n",
    "    })\n",
    "    panel[\"region\"] = panel[\"state\"].astype(\"category\")\n",
    "    panel[\"channel\"] = \"RETAIL\"\n",
    "    panel[\"StoreType\"] = panel[\"type\"].astype(\"category\")\n",
    "    panel[\"Assortment\"] = panel[\"cluster\"].astype(\"int16\")\n",
    "    panel[\"price_index\"] = 1.0\n",
    "    panel[\"SchoolHoliday\"] = 0\n",
    "    panel[\"StateHoliday\"] = panel[\"is_holiday\"]\n",
    "\n",
    "    keep = [\"date\",\"Store\",\"family\",\"region\",\"channel\",\"StoreType\",\"Assortment\",\n",
    "            \"Promo\",\"Sales\",\"price_index\"]\n",
    "    panel = panel[keep].sort_values([\"Store\",\"family\",\"date\"]).reset_index(drop=True)\n",
    "    return panel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1fe5b",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c6bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_lags(df: pd.DataFrame, tgt=\"Sales\", lags=(1,2,4,8,13,26,52), group=(\"Store\",\"family\")) -> pd.DataFrame:\n",
    "    d = df.sort_values(list(group)+[\"date\"]).copy()\n",
    "    for L in lags:\n",
    "        d[f\"lag_{L}\"] = d.groupby(list(group))[tgt].shift(L)\n",
    "    for w in (4,8,13,26,52):\n",
    "        d[f\"ma_{w}\"] = d.groupby(list(group))[tgt].transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "    d[\"week\"] = d[\"date\"].dt.isocalendar().week.astype(int)\n",
    "    d[\"month\"] = d[\"date\"].dt.month\n",
    "    d[\"year\"] = d[\"date\"].dt.year\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c20b68",
   "metadata": {},
   "source": [
    "## Modeling & backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19db5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def fit_models(train_df: pd.DataFrame, feature_cols: List[str], target=\"Sales\",\n",
    "               alpha_lo=0.10, alpha_hi=0.90) -> Dict[str, GradientBoostingRegressor]:\n",
    "    mdl_med = GradientBoostingRegressor(random_state=7)\n",
    "    mdl_lo = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha_lo, random_state=7)\n",
    "    mdl_hi = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha_hi, random_state=7)\n",
    "\n",
    "    X = train_df[feature_cols].fillna(train_df[feature_cols].median(numeric_only=True))\n",
    "    y = train_df[target].values\n",
    "\n",
    "    mdl_med.fit(X, y)\n",
    "    mdl_lo.fit(X, y)\n",
    "    mdl_hi.fit(X, y)\n",
    "    return {\"med\": mdl_med, \"lo\": mdl_lo, \"hi\": mdl_hi}\n",
    "\n",
    "def rolling_backtest(df: pd.DataFrame, feature_cols: List[str], target=\"Sales\",\n",
    "                     cutoff_weeks=16):\n",
    "    last_date = df[\"date\"].max()\n",
    "    cutoff = last_date - pd.Timedelta(weeks=cutoff_weeks)\n",
    "    tr = df[df[\"date\"] <= cutoff].copy()\n",
    "    te = df[df[\"date\"] > cutoff].copy()\n",
    "\n",
    "    models = fit_models(tr, feature_cols, target)\n",
    "    Xte = te[feature_cols].fillna(tr[feature_cols].median(numeric_only=True))\n",
    "    te[\"pred_med\"] = models[\"med\"].predict(Xte)\n",
    "\n",
    "    WAPE = float(np.sum(np.abs(te[target] - te[\"pred_med\"])) / np.sum(np.abs(te[target])) * 100)\n",
    "    MAPE = float(np.nanmean(np.where(te[target]==0, np.nan, np.abs((te[target] - te[\"pred_med\"]) / te[target]))) * 100)\n",
    "    RMSE = float(math.sqrt(mean_squared_error(te[target], te[\"pred_med\"])))\n",
    "    MAE = float(mean_absolute_error(te[target], te[\"pred_med\"]))\n",
    "    SMA = smape(te[target].values, te[\"pred_med\"].values)\n",
    "    MASE = mase(te[target].values, te[\"pred_med\"].values, tr[target].values)\n",
    "\n",
    "    metrics = {\"WAPE%\": WAPE, \"MAPE%\": MAPE, \"RMSE\": RMSE, \"MAE\": MAE, \"SMAPE%\": SMA, \"MASE\": MASE}\n",
    "    return te, metrics, models, tr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44778277",
   "metadata": {},
   "source": [
    "## Forecast & scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aaa51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forecast_future(hist_df: pd.DataFrame, models, horizon_weeks=52, scenario=\"BASE\"):\n",
    "    last_date = hist_df[\"date\"].max()\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta(weeks=1), periods=horizon_weeks, freq=\"W-SUN\")\n",
    "\n",
    "    rows = []\n",
    "    for (store, fam), base in hist_df.groupby([\"Store\",\"family\"]):\n",
    "        base = base.sort_values(\"date\").iloc[-1:]\n",
    "        for dt in future_dates:\n",
    "            rows.append({\n",
    "                \"Store\": store, \"family\": fam, \"date\": dt,\n",
    "                \"region\": base[\"region\"].values[0], \"channel\": base[\"channel\"].values[0],\n",
    "                \"StoreType\": base[\"StoreType\"].values[0], \"Assortment\": base[\"Assortment\"].values[0],\n",
    "                \"Promo\": 1 if (scenario==\"PROMO\" and (dt.isocalendar().week in [13,14,39,40])) else 0,\n",
    "                \"SchoolHoliday\": 0, \"StateHoliday\": 0,\n",
    "                \"price_index\": base[\"price_index\"].values[0]\n",
    "            })\n",
    "    fut = pd.DataFrame(rows)\n",
    "\n",
    "    if scenario == \"+2% PRICE\":\n",
    "        fut[\"price_index\"] = fut[\"price_index\"] * 1.02\n",
    "\n",
    "    hist_tail = hist_df[[\"Store\",\"family\",\"date\",\"Sales\",\"region\",\"channel\",\"StoreType\",\"Assortment\",\n",
    "                         \"Promo\",\"SchoolHoliday\",\"StateHoliday\",\"price_index\"]].copy()\n",
    "    full = pd.concat([hist_tail, fut], ignore_index=True).sort_values([\"Store\",\"family\",\"date\"])\n",
    "\n",
    "    full = make_lags(full, tgt=\"Sales\", lags=(1,2,4,8,13,26,52), group=(\"Store\",\"family\"))\n",
    "    feature_cols = [c for c in full.columns if c not in [\"Sales\",\"date\"] and full[c].dtype != \"O\"]\n",
    "\n",
    "    Xf = full[full[\"date\"].isin(fut[\"date\"])][feature_cols].fillna(full[feature_cols].median(numeric_only=True))\n",
    "    preds_med = models[\"med\"].predict(Xf)\n",
    "    preds_lo = models[\"lo\"].predict(Xf)\n",
    "    preds_hi = models[\"hi\"].predict(Xf)\n",
    "\n",
    "    out = fut.copy()\n",
    "    out[\"pred_med\"] = np.maximum(0, preds_med)\n",
    "    out[\"pred_lo\"] = np.maximum(0, np.minimum(preds_med, preds_lo))\n",
    "    out[\"pred_hi\"] = np.maximum(0, np.maximum(preds_med, preds_hi))\n",
    "\n",
    "    if scenario == \"SUPPLY CAP\":\n",
    "        mask = out[\"family\"]==\"AESTHETICS\"\n",
    "        out.loc[mask, [\"pred_med\",\"pred_lo\",\"pred_hi\"]] *= 0.9\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c7a2b",
   "metadata": {},
   "source": [
    "## Finance layer & aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d84cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finance_map(df_pred: pd.DataFrame, gtn=GTN_DEFAULT, cogs=COGS_DEFAULT) -> pd.DataFrame:\n",
    "    f = df_pred.copy()\n",
    "    f[\"net_revenue\"] = f[\"pred_med\"]\n",
    "    f[\"gross_revenue\"] = f[\"net_revenue\"] / (1 - gtn)\n",
    "    f[\"gtn_deduction\"] = f[\"gross_revenue\"] - f[\"net_revenue\"]\n",
    "    f[\"cogs\"] = f[\"net_revenue\"] * cogs\n",
    "    f[\"gross_margin\"] = f[\"net_revenue\"] - f[\"cogs\"]\n",
    "    f[\"contribution\"] = f[\"gross_margin\"]\n",
    "    return f\n",
    "\n",
    "def revenue_bridge(last_q: pd.DataFrame, next_q: pd.DataFrame) -> pd.DataFrame:\n",
    "    b = last_q.merge(next_q, on=\"family\", how=\"outer\", suffixes=(\"_last\",\"_next\")).fillna(0)\n",
    "    b[\"delta\"] = b[\"net_revenue_next\"] - b[\"net_revenue_last\"]\n",
    "    return b\n",
    "\n",
    "def agg_family(fin: pd.DataFrame) -> pd.DataFrame:\n",
    "    return fin.groupby([\"date\",\"family\"], as_index=False).agg(\n",
    "        net_revenue=(\"net_revenue\",\"sum\"),\n",
    "        gross_margin=(\"gross_margin\",\"sum\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c93b3",
   "metadata": {},
   "source": [
    "## PowerPoint export (12-slide outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d37d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_ppt(output_dir, dataset_name, metrics, top_family):\n",
    "    try:\n",
    "        from pptx import Presentation\n",
    "        from pptx.util import Inches, Pt\n",
    "        prs = Presentation()\n",
    "\n",
    "        def _add_title(title):\n",
    "            slide_layout = prs.slide_layouts[5]\n",
    "            slide = prs.slides.add_slide(slide_layout)\n",
    "            title_box = slide.shapes.title\n",
    "            if title_box is None:\n",
    "                title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(1))\n",
    "            title_box.text = title\n",
    "            title_box.text_frame.paragraphs[0].font.size = Pt(28)\n",
    "            title_box.text_frame.paragraphs[0].font.bold = True\n",
    "            return slide\n",
    "        def _add_bullets(slide, bullets):\n",
    "            left, top, width, height = Inches(0.7), Inches(1.2), Inches(8.6), Inches(5.0)\n",
    "            tx = slide.shapes.add_textbox(left, top, width, height).text_frame\n",
    "            tx.word_wrap = True\n",
    "            for i, b in enumerate(bullets):\n",
    "                p = tx.add_paragraph() if i>0 else tx.paragraphs[0]\n",
    "                p.text = b; p.level = 0; p.font.size = Pt(18)\n",
    "        def _add_picture(slide, img_path):\n",
    "            from pptx.util import Inches\n",
    "            slide.shapes.add_picture(img_path, Inches(0.7), Inches(1.2), width=Inches(8.6))\n",
    "\n",
    "        # Slides\n",
    "        s = _add_title(f\"{dataset_name.title()} Forecast & Finance — Results\")\n",
    "        _add_bullets(s, [\"Objective: Forecast sales and translate into P&L + S&OP for 4–12 quarters\",\n",
    "                         \"Method: GBRT + quantiles + scenarios\",\n",
    "                         \"Outputs: Forecast intervals, scenarios, GTN/COGS/GM, ROI, safety stock\"])\n",
    "\n",
    "        s = _add_title(\"Objective & Data\")\n",
    "        _add_bullets(s, [\"Scope: Store × Family weekly panel; horizon 52 weeks (configurable)\",\n",
    "                         \"Sources: Promotions, Holidays, Macro oil (Favorita), Store/Items metadata\",\n",
    "                         \"Caveats: Sales treated as Net Revenue; price index proxy; returns handled (Favorita)\"])\n",
    "\n",
    "        s = _add_title(\"Method (in one picture)\")\n",
    "        _add_bullets(s, [\"Features: calendar, promo recency, macro, lags/MAs\",\n",
    "                         \"Models: Gradient Boosting (median) + Quantile (10%/90%)\",\n",
    "                         \"Validation: rolling last-16 weeks backtest\"])\n",
    "\n",
    "        s = _add_title(\"Model Quality\")\n",
    "        _add_bullets(s, [f\"WAPE: {metrics.get('WAPE%', float('nan')):.2f}% | MAPE: {metrics.get('MAPE%', float('nan')):.2f}%\",\n",
    "                         f\"RMSE: {metrics.get('RMSE', float('nan')):.2f} | MAE: {metrics.get('MAE', float('nan')):.2f} | MASE: {metrics.get('MASE', float('nan')):.2f}\",\n",
    "                         \"Note: MAPE can look large on low-volume; prefer WAPE/MASE.\"])\n",
    "\n",
    "        s = _add_title(\"Drivers & Decomposition\")\n",
    "        _add_bullets(s, [\"Promo/holiday effects; macro oil (Favorita)\",\n",
    "                         \"Store/cluster/type as proxies for region/channel\",\n",
    "                         \"Trend/seasonality via calendar + lag features\"])\n",
    "\n",
    "        scen_plot = os.path.join(output_dir, \"plots\", f\"net_revenue_{top_family}.png\")\n",
    "        s = _add_title(f\"Base Forecast — {top_family}\")\n",
    "        if os.path.exists(scen_plot): _add_picture(s, scen_plot)\n",
    "\n",
    "        s = _add_title(\"Scenario Comparison\")\n",
    "        _add_bullets(s, [\"BASE vs PROMO vs +2% PRICE vs SUPPLY CAP\",\n",
    "                         \"Show Net Revenue & GM deltas by family/region\"])\n",
    "\n",
    "        bridge_plot = os.path.join(output_dir, \"plots\", \"revenue_bridge.png\")\n",
    "        s = _add_title(\"Revenue Bridge\")\n",
    "        if os.path.exists(bridge_plot): _add_picture(s, bridge_plot)\n",
    "\n",
    "        s = _add_title(\"Margin & Contribution\")\n",
    "        _add_bullets(s, [\"Apply GTN% to get Net Revenue; COGS → GM; Contribution = GM (variable costs can be added)\"])\n",
    "\n",
    "        roi_plot = os.path.join(output_dir, \"plots\", \"promo_roi_hist.png\")\n",
    "        s = _add_title(\"Promo ROI\")\n",
    "        if os.path.exists(roi_plot): _add_picture(s, roi_plot)\n",
    "\n",
    "        s = _add_title(\"Inventory & Service Level\")\n",
    "        _add_bullets(s, [\"Safety stock per family using z·σ·√lead_time\",\n",
    "                         \"Highlight stock-out hotspots & expiry risks if shelf-life is available\"])\n",
    "\n",
    "        s = _add_title(\"Actions & Owners\")\n",
    "        _add_bullets(s, [\"Scale high-ROI promos; stop negative-ROI campaigns\",\n",
    "                         \"Rebalance allocation to top-contribution families/stores\",\n",
    "                         \"Increase batch size for Aesthetics in Q2 to hit 95% service level\"])\n",
    "\n",
    "        out_ppt = os.path.join(output_dir, \"Forecast_Finance_Deck.pptx\")\n",
    "        prs.save(out_ppt)\n",
    "        return out_ppt\n",
    "    except Exception as e:\n",
    "        print(\"PPT generation error:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40826c",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ec7618",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Load\n",
    "if DATASET == 'rossmann':\n",
    "    raw = load_rossmann(ROSS_TRAIN, ROSS_STORE)\n",
    "    raw = add_calendar_features(raw, \"Date\", daily=True)\n",
    "    raw = add_promo_history(raw)\n",
    "    wk = weekly_panel_rossmann(raw)\n",
    "else:\n",
    "    wk = load_favorita(FAVORITA_DIR, since=FAVORITA_SINCE)\n",
    "\n",
    "wk.to_csv(os.path.join(OUTPUT_DIR, \"weekly_panel_model_ready.csv\"), index=False)\n",
    "wk.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff08834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Features & backtest\n",
    "wk = make_lags(wk, tgt=\"Sales\", lags=(1,2,4,8,13,26,52), group=(\"Store\",\"family\"))\n",
    "feat_cols = [\"Promo\",\"SchoolHoliday\",\"price_index\",\n",
    "             \"week\",\"month\",\"year\",\n",
    "             \"lag_1\",\"lag_2\",\"lag_4\",\"lag_8\",\"lag_13\",\"lag_26\",\"lag_52\",\n",
    "             \"ma_4\",\"ma_8\",\"ma_13\",\"ma_26\",\"ma_52\"]\n",
    "wk_model = wk.dropna(subset=[\"lag_1\",\"lag_2\",\"lag_4\"]).copy()\n",
    "\n",
    "test_df, metrics, models, train_df = rolling_backtest(wk_model, feat_cols, target=\"Sales\", cutoff_weeks=16)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Forecast scenarios\n",
    "fc_base   = forecast_future(wk_model, models, horizon_weeks=HORIZON_WEEKS, scenario=\"BASE\")\n",
    "fc_promo  = forecast_future(wk_model, models, horizon_weeks=HORIZON_WEEKS, scenario=\"PROMO\")\n",
    "fc_price  = forecast_future(wk_model, models, horizon_weeks=HORIZON_WEEKS, scenario=\"+2% PRICE\")\n",
    "fc_supply = forecast_future(wk_model, models, horizon_weeks=HORIZON_WEEKS, scenario=\"SUPPLY CAP\")\n",
    "\n",
    "# 4) Finance\n",
    "fin_base   = finance_map(fc_base, gtn=GTN_DEFAULT, cogs=COGS_DEFAULT)\n",
    "fin_promo  = finance_map(fc_promo, gtn=GTN_DEFAULT, cogs=COGS_DEFAULT)\n",
    "fin_price  = finance_map(fc_price, gtn=GTN_DEFAULT, cogs=COGS_DEFAULT)\n",
    "fin_supply = finance_map(fc_supply, gtn=GTN_DEFAULT, cogs=COGS_DEFAULT)\n",
    "\n",
    "def agg_family(fin):\n",
    "    return fin.groupby([\"date\",\"family\"], as_index=False).agg(\n",
    "        net_revenue=(\"net_revenue\",\"sum\"),\n",
    "        gross_margin=(\"gross_margin\",\"sum\"))\n",
    "\n",
    "dash_all = pd.concat([\n",
    "    agg_family(fin_base).assign(scenario=\"BASE\"),\n",
    "    agg_family(fin_promo).assign(scenario=\"PROMO\"),\n",
    "    agg_family(fin_price).assign(scenario=\"+2% PRICE\"),\n",
    "    agg_family(fin_supply).assign(scenario=\"SUPPLY CAP\")\n",
    "], ignore_index=True)\n",
    "\n",
    "# 5) Revenue bridge\n",
    "hist_last_q = (wk_model[wk_model[\"date\"] > wk_model[\"date\"].max()-pd.Timedelta(weeks=13)]\n",
    "               .groupby(\"family\", as_index=False)[\"Sales\"].sum()\n",
    "               .rename(columns={\"Sales\":\"net_revenue\"}))\n",
    "next_q = fin_base.groupby(\"family\", as_index=False)[\"net_revenue\"].sum()\n",
    "bridge = revenue_bridge(hist_last_q, next_q)\n",
    "\n",
    "# 6) Promo ROI vs base\n",
    "cmp_cols = [\"Store\",\"family\",\"date\",\"net_revenue\",\"cogs\",\"gross_margin\"]\n",
    "base_cmp = fin_base[cmp_cols].rename(columns={\"net_revenue\":\"base_nr\",\"cogs\":\"base_cogs\",\"gross_margin\":\"base_gm\"})\n",
    "promo_cmp = fin_promo[cmp_cols].merge(base_cmp, on=[\"Store\",\"family\",\"date\"], how=\"left\")\n",
    "promo_cmp[\"incr_nr\"] = promo_cmp[\"net_revenue\"] - promo_cmp[\"base_nr\"]\n",
    "promo_cmp[\"incr_gp\"] = (promo_cmp[\"gross_margin\"]) - (promo_cmp[\"base_gm\"])\n",
    "promo_cmp[\"promo_cost\"] = np.where(promo_cmp[\"incr_nr\"]>0, PROMO_COST_RATE*promo_cmp[\"incr_nr\"], 0.0)\n",
    "promo_cmp[\"ROI\"] = np.where(promo_cmp[\"promo_cost\"]>0, promo_cmp[\"incr_gp\"]/promo_cmp[\"promo_cost\"], np.nan)\n",
    "\n",
    "# 7) Inventory safety stock (per family)\n",
    "resid = test_df[\"Sales\"] - test_df[\"pred_med\"]\n",
    "sigma = float(np.std(resid))\n",
    "safety_rows = []\n",
    "for fam, lead in LEAD_WEEKS.items():\n",
    "    ss = SERVICE_LEVEL_Z * max(1.0, sigma) * math.sqrt(lead)\n",
    "    safety_rows.append({\"family\": fam, \"lead_weeks\": lead, \"safety_stock_units\": ss})\n",
    "safety_df = pd.DataFrame(safety_rows)\n",
    "\n",
    "# Persist tables\n",
    "ensure_dir(os.path.join(OUTPUT_DIR, \"plots\"))\n",
    "pd.DataFrame([metrics]).to_csv(os.path.join(OUTPUT_DIR, \"backtest_metrics.csv\"), index=False)\n",
    "dash_all.to_csv(os.path.join(OUTPUT_DIR, \"scenario_family_week.csv\"), index=False)\n",
    "promo_cmp.groupby([\"Store\",\"family\"], as_index=False)[[\"ROI\",\"incr_gp\",\"promo_cost\"]].sum().to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"promo_roi_by_store.csv\"), index=False)\n",
    "safety_df.to_csv(os.path.join(OUTPUT_DIR, \"safety_stock.csv\"), index=False)\n",
    "\n",
    "dash_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45e72b",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ec2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_fam = dash_all[dash_all[\"scenario\"]==\"BASE\"].groupby(\"family\")[\"net_revenue\"].sum().sort_values(ascending=False).index[0]\n",
    "\n",
    "# Scenario line chart\n",
    "plt.figure()\n",
    "for scen, d in dash_all[dash_all[\"family\"]==top_fam].groupby(\"scenario\"):\n",
    "    plt.plot(d[\"date\"], d[\"net_revenue\"], label=scen)\n",
    "plt.legend(); plt.title(f\"Net Revenue Forecast — {top_fam}\")\n",
    "plt.xlabel(\"Week\"); plt.ylabel(\"Net Revenue\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", f\"net_revenue_{top_fam}.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Revenue bridge (bar)\n",
    "plt.figure()\n",
    "plt.bar(bridge[\"family\"], bridge[\"net_revenue_last\"], label=\"Last Q\")\n",
    "plt.bar(bridge[\"family\"], bridge[\"delta\"], bottom=bridge[\"net_revenue_last\"], label=\"Δ to Next Q (Base)\")\n",
    "plt.legend(); plt.title(\"Revenue Bridge: Last Quarter → Next Quarter (Base)\")\n",
    "plt.xlabel(\"Family\"); plt.ylabel(\"Net Revenue\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"revenue_bridge.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Promo ROI histogram\n",
    "plt.figure()\n",
    "plt.hist(promo_cmp[\"ROI\"].dropna(), bins=30)\n",
    "plt.title(\"Promo ROI distribution (by Store×Family)\")\n",
    "plt.xlabel(\"ROI (Gross Profit / Promo Cost)\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"promo_roi_hist.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333f62a",
   "metadata": {},
   "source": [
    "## Build PowerPoint deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install python-pptx --quiet\n",
    "metrics_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"backtest_metrics.csv\")).iloc[0].to_dict()\n",
    "ppt_path = build_ppt(OUTPUT_DIR, DATASET, metrics_df, top_fam)\n",
    "ppt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9370f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Forest-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
